{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training Neural Network ",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/cheonbok94/DeepLearningLecture/blob/master/Training_Neural_Network.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "4oOxnJsphGb3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch \n",
        "import torch.nn as nn #\n",
        "import torch.nn.functional as F #\n",
        "import torchvision # 이미지 관련 처리, Pretrained Model 관련된 Package 입니다. \n",
        "import torchvision.datasets as vision_dsets\n",
        "import torchvision.transforms as T # 이미지 처리 (Vison) 관련된 transformation이 정의 되어 있습니다.\n",
        "import torch.optim as optim # pytorch 에서 정의한 수 많은 optimization function 들이 들어 있습니다.\n",
        "from torch.autograd import Variable \n",
        "from torch.utils import data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9ABEFSDDZ8ON",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# MNIST Feed-forward Neural Network "
      ]
    },
    {
      "metadata": {
        "id": "qije6EXLt9sZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data Loader 불러오기"
      ]
    },
    {
      "metadata": {
        "id": "Yo1YYiybty5e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def MNIST_DATA(root='./',train =True,transforms=None ,download =True,batch_size = 32,num_worker = 1):\n",
        "\n",
        "\tprint (\"[+] Get the MNIST DATA\")\n",
        "\t\"\"\"\n",
        "  \ttorchvision.dataset 에는 우리가 많이 사용하는 데이터들을 쉽게 사용할 수 있도록 되어 있습니다. \n",
        "  \tMachine Learning 에서 Hello world 라고 불리는 Mnist 데이터를 사용해 보겠습니다. \n",
        "  \n",
        "  \n",
        "\t\"\"\"\n",
        "\tmnist_train = vision_dsets.MNIST(root = root,  #root 는 데이터의 저장 위치 입니다. \n",
        "\t\t\t\t\t\t\t\t\ttrain = True, #Train 은 이 데이터가 train 데이터인지 아닌지에 대한 정보입니다. \n",
        "\t\t\t\t\t\t\t\t\ttransform = T.ToTensor(), # 얻어낸 데이터를 pytorch가 계산 할 수 있는 Tensor 로 변환해 줍니다. \n",
        "\t\t\t\t\t\t\t\t\tdownload = True)  # 데이터를 다운로드 할지 여부를 물어봅니다. \n",
        "\tmnist_test = vision_dsets.MNIST(root = root,\n",
        "\t\t\t\t\t\t\t\t\ttrain = False,  # Test Data를 가져오기에 Train =False 를 줘야 합니다. \n",
        "\t\t\t\t\t\t\t\t\ttransform = T.ToTensor(),\n",
        "\t\t\t\t\t\t\t\t\tdownload = True)\n",
        "\t\"\"\"\n",
        "  \tData Loader 는 데이터와 batch size의 정보를 바탕으로 매 iteration 마다 주어진 데이터를 원하는 batch size 만큼 반환해주는 iterator입니다. \n",
        "  \t* Practical Guide : Batch size 는 어느정도가 좋나요? -- 클 수록 좋다는 소리가 있습니다. 하지만 gpu memeory 사이즈 한계에 의해 기본적으로 batch size 가 \n",
        "  \t커질 수록 학습에 사용되는 gpu memory 사이즈가 큽니다. (Activation map을 저장해야 하기 때문입니다.) 기본적으로 2의 배수로 저장하는 것이 좋습니다.(Bit size 관련) \n",
        "  \n",
        "\t\"\"\"\n",
        "\ttrainDataLoader = data.DataLoader(dataset = mnist_train,  # DataSet은 어떤 Data를 제공해 줄지에 대한 정보입니다. 여기서는 Training DATA를 제공합니다. \n",
        "\t\t\t\t\t\t\t\t\tbatch_size = batch_size, # batch size 정보를 꼭 줘야 합니다. 한 Batch 당 몇 개의 Data 를 제공할지에 대한 정보입니다. \n",
        "\t\t\t\t\t\t\t\t\tshuffle =True, # Training의 경우 Shuffling 을 해주는 것이 성능에 지대한 영향을 끼칩니다. 꼭 True 를 줘야 합니다. \n",
        "\t\t\t\t\t\t\t\t\tnum_workers = 1) # num worker의 경우 데이터를 로드하는데 worker를 얼마나 추가하겠는가에 대한 정보입니다. \n",
        "\n",
        "\ttestDataLoader = data.DataLoader(dataset = mnist_test, # Test Data Loader 이므로 Test Data를 인자로 전달해줍니다.\n",
        "\t\t\t\t\t\t\t\t\tbatch_size = batch_size, # 마찬가지로 Batch size 를 넣어줍니다. \n",
        "\t\t\t\t\t\t\t\t\tshuffle = False, # shuffling 이 굳이 필요하지 않으므로 false를 줍니다. \n",
        "\t\t\t\t\t\t\t\t\tnum_workers = 1) #\n",
        "\tprint (\"[+] Finished loading data & Preprocessing\")\n",
        "\treturn mnist_train,mnist_test,trainDataLoader,testDataLoader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_P2oJog_xnFJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "17b3d808-dd30-458c-8c08-8152f669f970"
      },
      "cell_type": "code",
      "source": [
        "trainDset,testDset,trainDataLoader,testDataLoader= MNIST_DATA(batch_size = 32)  # Data Loader 를 불러 옵니다. "
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[+] Get the MNIST DATA\n",
            "[+] Finished loading data & Preprocessing\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vtWA2FTIxskN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train Function "
      ]
    },
    {
      "metadata": {
        "id": "PA8glX3ox0Hh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_network(net,optimizer,trainloader):\n",
        "  for epoch in range(4):  # loop over the dataset multiple times\n",
        "\n",
        "      running_loss = 0.0 # running loss를 저장하기 위한 변수입니다. \n",
        "      for i, data in enumerate(trainloader, 0): # 한 Epoch 만큼 돕니다. 매 iteration 마다 정해진 Batch size 만큼 데이터를 뱉습니다. \n",
        "          # get the inputs\n",
        "          inputs, labels = data # DataLoader iterator의 반환 값은 input_data 와 labels의 튜플 형식입니다. \n",
        "          inputs = Variable(inputs).cuda() # Pytorch에서 nn.Module 에 넣어 Backprop을 계산 하기 위해서는 Variable로 감싸야 합니다.\n",
        "          labels = Variable(labels).cuda()\n",
        "          # zero the parameter gradients\n",
        "          optimizer.zero_grad()    #  현재 기존의 backprop을 계산하기 위해서 저장했던 activation buffer 를 비웁니다. Q) 이걸 안 한다면?\n",
        "\n",
        "          # forward + backward + optimize\n",
        "          outputs = net(inputs) # input 을 넣은 위 network 로 부터 output 을 얻어냅니다. \n",
        "          loss = criterion(outputs, labels) # loss fucntion에 주어진 target과 output 의 score를 계산하여 반환합니다. \n",
        "          loss.backward() # * Scalar Loss value를 Backward() 해주게 되면 주어진 loss값을 바탕으로 backpropagation이 진행됩니다. \n",
        "          optimizer.step() # 계산된 Backprop 을 바탕으로 optimizer가 gradient descenting 을 수행합니다. \n",
        "\n",
        "          # print statistics\n",
        "          running_loss += loss.data[0]\n",
        "          if i % 500 == 499:    # print every 2000 mini-batches\n",
        "              print('[%d, %5d] loss: %.3f' %\n",
        "                    (epoch + 1, i + 1, running_loss / 500))\n",
        "              running_loss = 0.0\n",
        "\n",
        "  print('Finished Training')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zGMkGKQiy8_r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Test Function"
      ]
    },
    {
      "metadata": {
        "id": "X0Xn-H2zbc22",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test(model,test_loader):\n",
        "  model.eval() # Eval Mode 왜 해야 할까요?  --> nn.Dropout BatchNorm 등의 Regularization 들이 test 모드로 들어가게 되기 때문입니다. \n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  for data, target in test_loader:\n",
        "    data, target = Variable(data).cuda(), Variable(target).cuda()  # 기존의 train function의 data 처리부분과 같습니다. \n",
        "    output = model(data) \n",
        "    pred = output.max(1, keepdim=True)[1] # get the index of the max \n",
        "    correct += pred.eq(target.view_as(pred)).sum().data[0] # 정답 데이터의 갯수를 반환합니다. \n",
        "\n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  print('\\nTest set:  Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "      correct, len(test_loader.dataset),\n",
        "      100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mdM-bsXD152f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Neural Network  + Activation Function"
      ]
    },
    {
      "metadata": {
        "id": "fmv3W0Ln29M1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 간단한 Neural Network 를 만들어 봅시다. (1)\n",
        "특징 : 2개의 Layer를 가지는 Neural Network \n",
        "<구성>  \n",
        "Layer 1 - input:28*28 , output : 30 + Activation Fucntion - Sigmoid \n",
        "\n",
        "Layer 2 - input: 30 output:10\n",
        "\n",
        "Cross Entropy Loss  + SGD optimizer "
      ]
    },
    {
      "metadata": {
        "id": "MTJOsz9U2FnP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "class MNIST_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNIST_Net, self).__init__() # nn.Module 생성자 호출 Q) 왜 필요할까요?\n",
        "        # an affine operation: y = Wx + b\n",
        "        self.fc0 = nn.Linear(28*28,30)\n",
        "        self.fc1 = nn.Linear(30, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1,28*28) # x.view함수는 주어진 인자의 크기로 해당 데이터의 크기를 반환합니다. 즉, (Batch_size,28,28) --> (Batch_size,28*28)로 변환합니다.\n",
        "        x = F.sigmoid(self.fc0(x)) # 28*28 -> 30 -> Activation function 을 수행합니다.\n",
        "        x = self.fc1(x)  # 30 -> 10 으로 10개의 Class에 대한 logit 값을 호출합니다. \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Bvk4TTL83pcc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Optimizer \n",
        "Optimizer 의 경우 기본적으로 torch.optim 안에 존재합니다. 다양한 optimziers 가 정의되어 있습니다. \n",
        "\n",
        "기본적으로 다음과 같은 구성을 따릅니다. optim.{Optimzier 이름}({Network Parameters},lr ={learning rate })"
      ]
    },
    {
      "metadata": {
        "id": "wd5tmyMnXcs0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mnist_net = MNIST_Net().cuda() # 생성한 뉴럴넷 Instance를 생성하고 빠른 학습을 위해 cuda 에 올립니다. \n",
        "criterion = nn.CrossEntropyLoss() # Loss Function을 정의 합니다. 여기서는 cross entrophy loss 를 사용합니다. \n",
        "optimizer = optim.SGD(mnist_net.parameters(), lr=0.001) # optimizer는 이와 같이 training 할 Parameter와 learning rate를 인자로 줍니다. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zALyUje-aehn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "2f40eb69-e200-4b3d-f842-50e337abff09"
      },
      "cell_type": "code",
      "source": [
        "train_network(mnist_net,optimizer,trainDataLoader) # 4 Epoch 정도 학습을 진행해봅니다. "
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,   500] loss: 2.366\n",
            "[1,  1000] loss: 2.313\n",
            "[1,  1500] loss: 2.285\n",
            "[2,   500] loss: 2.256\n",
            "[2,  1000] loss: 2.243\n",
            "[2,  1500] loss: 2.230\n",
            "[3,   500] loss: 2.205\n",
            "[3,  1000] loss: 2.194\n",
            "[3,  1500] loss: 2.177\n",
            "[4,   500] loss: 2.149\n",
            "[4,  1000] loss: 2.133\n",
            "[4,  1500] loss: 2.118\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LfQrMIX2a3nn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "2d790e34-f41c-489f-8988-a9decba57567"
      },
      "cell_type": "code",
      "source": [
        "test(mnist_net,testDataLoader) # Test 정확도를 출력해 봅니다. "
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set:  Accuracy: 6152/10000 (62%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DUK2GbHZ4x-K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 간단한 Neural Network 를 만들어 봅시다. (2)\n",
        "특징 : 2개의 Layer를 가지는 Neural Network \n",
        "<구성>  \n",
        "Layer 1 - input:28*28 , output : 30 + Activation Fucntion - tanh \n",
        "\n",
        "Layer 2 - input: 30 output:10\n",
        "\n",
        "Cross Entropy Loss  + SGD optimizer "
      ]
    },
    {
      "metadata": {
        "id": "FZUmc76ZdOap",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "class MNIST_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNIST_Net, self).__init__() # nn.Module 생성자 호출 Q) 왜 필요할까요?\n",
        "        # an affine operation: y = Wx + b\n",
        "        self.fc0 = nn.Linear(28*28,30)\n",
        "        self.fc1 = nn.Linear(30, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1,28*28) # x.view함수는 주어진 인자의 크기로 해당 데이터의 크기를 반환합니다. 즉, (Batch_size,28,28) --> (Batch_size,28*28)로 변환합니다.\n",
        "        x = F.tanh(self.fc0(x)) # 28*28 -> 30 -> Activation function 을 수행합니다.\n",
        "        x = self.fc1(x)  # 30 -> 10 으로 10개의 Class에 대한 logit 값을 호출합니다. \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UBXketXnhA_E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mnist_net = MNIST_Net().cuda() # 생성한 뉴럴넷 Instance를 생성하고 빠른 학습을 위해 cuda 에 올립니다. \n",
        "criterion = nn.CrossEntropyLoss() # Loss Function을 정의 합니다. 여기서는 cross entrophy loss 를 사용합니다. \n",
        "optimizer = optim.SGD(mnist_net.parameters(), lr=0.001) # optimizer는 이와 같이 training 할 Parameter와 learning rate를 인자로 줍니다. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Qb3e4wqphDmZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "f3027e4b-a767-405d-eb61-69c09270101f"
      },
      "cell_type": "code",
      "source": [
        "train_network(mnist_net,optimizer,trainDataLoader)"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,   500] loss: 2.222\n",
            "[1,  1000] loss: 2.078\n",
            "[1,  1500] loss: 1.934\n",
            "[2,   500] loss: 1.700\n",
            "[2,  1000] loss: 1.581\n",
            "[2,  1500] loss: 1.464\n",
            "[3,   500] loss: 1.301\n",
            "[3,  1000] loss: 1.211\n",
            "[3,  1500] loss: 1.146\n",
            "[4,   500] loss: 1.028\n",
            "[4,  1000] loss: 0.978\n",
            "[4,  1500] loss: 0.942\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZJ-WCpKrhEzR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "de4d32d3-f0b5-4235-b1e5-d7c228b13dc7"
      },
      "cell_type": "code",
      "source": [
        "test(mnist_net,testDataLoader)"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set:  Accuracy: 8327/10000 (83%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IV_aGry05TIV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 간단한 Neural Network 를 만들어 봅시다. (3)\n",
        "특징 : 2개의 Layer를 가지는 Neural Network \n",
        "<구성>  \n",
        "Layer 1 - input:28*28 , output : 30 + Activation Fucntion - Relu\n",
        "\n",
        "Layer 2 - input: 30 output:10\n",
        "\n",
        "Cross Entropy Loss  + SGD optimizer "
      ]
    },
    {
      "metadata": {
        "id": "PvAhsyP-hbwx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MNIST_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNIST_Net, self).__init__()\n",
        "        # an affine operation: y = Wx + b\n",
        "        self.fc0 = nn.Linear(28*28,30)\n",
        "        self.fc1 = nn.Linear(30, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Max pooling over a (2, 2) window\n",
        "        # If the size is a square you can only specify a single number\n",
        "        x = x.view(-1,28*28)\n",
        "        x = F.relu(self.fc0(x))\n",
        "        x = self.fc1(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mZVpqqCbhiNd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mnist_net = MNIST_Net().cuda() # 생성한 뉴럴넷 Instance를 생성하고 빠른 학습을 위해 cuda 에 올립니다. \n",
        "criterion = nn.CrossEntropyLoss() # Loss Function을 정의 합니다. 여기서는 cross entrophy loss 를 사용합니다. \n",
        "optimizer = optim.SGD(mnist_net.parameters(), lr=0.001) # optimizer는 이와 같이 training 할 Parameter와 learning rate를 인자로 줍니다. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fKGyPdSIhmqx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "9dd74f82-702f-4d0e-b830-d8b8f94a417a"
      },
      "cell_type": "code",
      "source": [
        "train_network(mnist_net,optimizer,trainDataLoader)"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,   500] loss: 2.269\n",
            "[1,  1000] loss: 2.185\n",
            "[1,  1500] loss: 2.071\n",
            "[2,   500] loss: 1.838\n",
            "[2,  1000] loss: 1.685\n",
            "[2,  1500] loss: 1.538\n",
            "[3,   500] loss: 1.297\n",
            "[3,  1000] loss: 1.169\n",
            "[3,  1500] loss: 1.064\n",
            "[4,   500] loss: 0.920\n",
            "[4,  1000] loss: 0.862\n",
            "[4,  1500] loss: 0.808\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1_tEaqWlhnUQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "7905d63d-f7dd-459d-dcb7-a159f6967057"
      },
      "cell_type": "code",
      "source": [
        "test(mnist_net,testDataLoader)"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set:  Accuracy: 8473/10000 (85%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FfWtUiir7XAK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Q) 성능차이가 존재하나요? 존재한다면 무슨 이유일까요?"
      ]
    },
    {
      "metadata": {
        "id": "FQALVsvI5ful",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 간단한 Neural Network 를 만들어 봅시다. (4) \n",
        "특징 : 3개의 Layer를 가지는 Neural Network \n",
        "<구성>  \n",
        "Layer 1 - input:28*28 , output : 40 + Activation Fucntion - sigmoid \n",
        "\n",
        "Layer 2 - input: 40 output: 30\n",
        "\n",
        "Layer 3 - input: 30 output : 10\n",
        "\n",
        "Cross Entropy Loss  + SGD optimizer "
      ]
    },
    {
      "metadata": {
        "id": "TLEQ6RZXjOCi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MNIST_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNIST_Net, self).__init__()\n",
        "        self.fc0 = nn.Linear(28*28,40) # Layer 1\n",
        "        self.fc1 = nn.Linear(40, 30) # Layer 2\n",
        "        self.fc2 = nn.Linear(30, 10) # Layer 3\n",
        "\n",
        "    def forward(self, x):\n",
        "      \n",
        "        x = x.view(-1,28*28)\n",
        "        x = F.sigmoid(self.fc0(x))\n",
        "        x = F.sigmoid(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1MM_WIY6jVG6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mnist_net = MNIST_Net().cuda() # 생성한 뉴럴넷 Instance를 생성하고 빠른 학습을 위해 cuda 에 올립니다. \n",
        "criterion = nn.CrossEntropyLoss() # Loss Function을 정의 합니다. 여기서는 cross entrophy loss 를 사용합니다. \n",
        "optimizer = optim.SGD(mnist_net.parameters(), lr=0.001) # optimizer는 이와 같이 training 할 Parameter와 learning rate를 인자로 줍니다. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3kWON9jtjXAy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "1f00680f-4582-4351-8ec7-dd6f671ff6af"
      },
      "cell_type": "code",
      "source": [
        "train_network(mnist_net,optimizer,trainDataLoader)"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,   500] loss: 2.317\n",
            "[1,  1000] loss: 2.307\n",
            "[1,  1500] loss: 2.304\n",
            "[2,   500] loss: 2.300\n",
            "[2,  1000] loss: 2.301\n",
            "[2,  1500] loss: 2.301\n",
            "[3,   500] loss: 2.300\n",
            "[3,  1000] loss: 2.300\n",
            "[3,  1500] loss: 2.300\n",
            "[4,   500] loss: 2.299\n",
            "[4,  1000] loss: 2.299\n",
            "[4,  1500] loss: 2.298\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OaCuXaHWjXzH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "a91a46c4-726f-4658-e900-abaad4b4ed21"
      },
      "cell_type": "code",
      "source": [
        "test(mnist_net,testDataLoader)"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set:  Accuracy: 1135/10000 (11%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hxqP1tY965JF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Q) 학습이 잘 되나요???? 안 된다면 왜 안될까요?"
      ]
    },
    {
      "metadata": {
        "id": "JZIkQe9f6tdy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 간단한 Neural Network 를 만들어 봅시다. (5) \n",
        "특징 : 3개의 Layer를 가지는 Neural Network \n",
        "<구성>  \n",
        "Layer 1 - input:28*28 , output : 40 + Activation Fucntion - Relu \n",
        "\n",
        "Layer 2 - input: 40 output: 30\n",
        "\n",
        "Layer 3 - input: 30 output : 10\n",
        "\n",
        "Cross Entropy Loss  + SGD optimizer "
      ]
    },
    {
      "metadata": {
        "id": "0uHrf8bRjZFw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MNIST_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNIST_Net, self).__init__()\n",
        "        # an affine operation: y = Wx + b\n",
        "        self.fc0 = nn.Linear(28*28,40) #Layer 1 \n",
        "        self.fc1 = nn.Linear(40, 30) # Layer 2\n",
        "        self.fc2 = nn.Linear(30, 10) # Layer 3\n",
        "\n",
        "    def forward(self, x):\n",
        "       \n",
        "        x = x.view(-1,28*28)\n",
        "        x = F.relu(self.fc0(x)) # Layer 1\n",
        "        x = F.relu(self.fc1(x)) # Layer 2\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kEtXES71j9s6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mnist_net = MNIST_Net().cuda() # 생성한 뉴럴넷 Instance를 생성하고 빠른 학습을 위해 cuda 에 올립니다. \n",
        "criterion = nn.CrossEntropyLoss() # Loss Function을 정의 합니다. 여기서는 cross entrophy loss 를 사용합니다. \n",
        "optimizer = optim.SGD(mnist_net.parameters(), lr=0.001) # optimizer는 이와 같이 training 할 Parameter와 learning rate를 인자로 줍니다. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IxHiSRSskasK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "93298acd-c84b-44b5-91c9-1e20e70d538c"
      },
      "cell_type": "code",
      "source": [
        "train_network(mnist_net,optimizer,trainDataLoader)"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,   500] loss: 2.306\n",
            "[1,  1000] loss: 2.295\n",
            "[1,  1500] loss: 2.283\n",
            "[2,   500] loss: 2.255\n",
            "[2,  1000] loss: 2.233\n",
            "[2,  1500] loss: 2.206\n",
            "[3,   500] loss: 2.137\n",
            "[3,  1000] loss: 2.085\n",
            "[3,  1500] loss: 2.017\n",
            "[4,   500] loss: 1.854\n",
            "[4,  1000] loss: 1.736\n",
            "[4,  1500] loss: 1.613\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0woJ7sbzkbqj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "c05efe2c-723c-4c0d-e625-78d6942a0c57"
      },
      "cell_type": "code",
      "source": [
        "test(mnist_net,testDataLoader)"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set:  Accuracy: 6856/10000 (69%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5YCjGFh77xeq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### (4)와 차이가 존재하나요? 그렇다면 왜 그럴까요? (3) 이랑은 비교해보면 어떻나요? "
      ]
    },
    {
      "metadata": {
        "id": "ezH4C21Y8JF4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 간단한 Neural Network 를 만들어 봅시다. (6) \n",
        "특징 : 3개의 Layer를 가지는 Neural Network \n",
        "<구성>  \n",
        "Layer 1 - input:28*28 , output : 40 + Activation Fucntion - Relu \n",
        "\n",
        "Layer 2 - input: 40 output: 30\n",
        "\n",
        "Layer 3 - input: 30 output : 10\n",
        "\n",
        "Cross Entropy Loss  + **Adam** optimizer "
      ]
    },
    {
      "metadata": {
        "id": "l-oDSJBL8Wk5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MNIST_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNIST_Net, self).__init__()\n",
        "        # an affine operation: y = Wx + b\n",
        "        self.fc0 = nn.Linear(28*28,40) #Layer 1 \n",
        "        self.fc1 = nn.Linear(40, 30) # Layer 2\n",
        "        self.fc2 = nn.Linear(30, 10) # Layer 3\n",
        "\n",
        "    def forward(self, x):\n",
        "       \n",
        "        x = x.view(-1,28*28)\n",
        "        x = F.relu(self.fc0(x)) # Layer 1\n",
        "        x = F.relu(self.fc1(x)) # Layer 2\n",
        "        x = self.fc2(x) # Layer 3 \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EJEAkxqX8YWR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mnist_net = MNIST_Net().cuda() # 생성한 뉴럴넷 Instance를 생성하고 빠른 학습을 위해 cuda 에 올립니다. \n",
        "criterion = nn.CrossEntropyLoss() # Loss Function을 정의 합니다. 여기서는 cross entrophy loss 를 사용합니다. \n",
        "optimizer = optim.Adam(mnist_net.parameters(), lr=0.001) # optimizer는 이와 같이 training 할 Parameter와 learning rate를 인자로 줍니다. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AVGBPs3W8dRd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "089193da-dbd1-4957-8ce4-15a3be1a1c5a"
      },
      "cell_type": "code",
      "source": [
        "train_network(mnist_net,optimizer,trainDataLoader)"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,   500] loss: 0.702\n",
            "[1,  1000] loss: 0.322\n",
            "[1,  1500] loss: 0.276\n",
            "[2,   500] loss: 0.217\n",
            "[2,  1000] loss: 0.192\n",
            "[2,  1500] loss: 0.191\n",
            "[3,   500] loss: 0.160\n",
            "[3,  1000] loss: 0.154\n",
            "[3,  1500] loss: 0.145\n",
            "[4,   500] loss: 0.123\n",
            "[4,  1000] loss: 0.126\n",
            "[4,  1500] loss: 0.123\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PKcDCwPJ8dqP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "808e3b59-1787-4bc9-eb83-7366d4b48e01"
      },
      "cell_type": "code",
      "source": [
        "test(mnist_net,testDataLoader)"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set:  Accuracy: 9604/10000 (96%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "P7_NK6ue8t6z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 간단한 Neural Network 를 만들어 봅시다. (7) Layer 를 줄여볼까요? \n",
        "특징 : 2개의 Layer를 가지는 Neural Network \n",
        "<구성>  \n",
        "Layer 1 - input:28*28 , output : 30 + Activation Fucntion - Relu \n",
        "\n",
        "Layer 2 - input: 30 output : 10\n",
        "\n",
        "Cross Entropy Loss  + **Adam** optimizer "
      ]
    },
    {
      "metadata": {
        "id": "QjnnxkiG8slU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MNIST_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNIST_Net, self).__init__()\n",
        "        # an affine operation: y = Wx + b\n",
        "        self.fc0 = nn.Linear(28*28,30) #Layer 1 \n",
        "        self.fc1 =  nn.Linear(30, 10) # Layer 2\n",
        "\n",
        "    def forward(self, x):\n",
        "       \n",
        "        x = x.view(-1,28*28)\n",
        "        x = F.relu(self.fc0(x)) # Layer 1\n",
        "        x = self.fc1(x) # Layer 2\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nQea1DZS8s2E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mnist_net = MNIST_Net().cuda() # 생성한 뉴럴넷 Instance를 생성하고 빠른 학습을 위해 cuda 에 올립니다. \n",
        "criterion = nn.CrossEntropyLoss() # Loss Function을 정의 합니다. 여기서는 cross entrophy loss 를 사용합니다. \n",
        "optimizer = optim.Adam(mnist_net.parameters(), lr=0.001) # optimizer는 이와 같이 training 할 Parameter와 learning rate를 인자로 줍니다. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-wPfgH1S8tFP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "8ab31da8-77e1-43a4-f6ab-23280d4bfde8"
      },
      "cell_type": "code",
      "source": [
        "train_network(mnist_net,optimizer,trainDataLoader)"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,   500] loss: 0.669\n",
            "[1,  1000] loss: 0.329\n",
            "[1,  1500] loss: 0.293\n",
            "[2,   500] loss: 0.241\n",
            "[2,  1000] loss: 0.223\n",
            "[2,  1500] loss: 0.209\n",
            "[3,   500] loss: 0.173\n",
            "[3,  1000] loss: 0.182\n",
            "[3,  1500] loss: 0.161\n",
            "[4,   500] loss: 0.144\n",
            "[4,  1000] loss: 0.142\n",
            "[4,  1500] loss: 0.137\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_8vW8d-18-Ix",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "a506c72b-9224-46f3-efc8-879ab1b75b0c"
      },
      "cell_type": "code",
      "source": [
        "test(mnist_net,testDataLoader)"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set:  Accuracy: 9587/10000 (96%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nKbhGSix9UQM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 간단한 Neural Network 를 만들어 봅시다. (7) Batch Norm 을 줘 볼까요?\n",
        "특징 : 2개의 Layer를 가지는 Neural Network \n",
        "<구성>  \n",
        "Layer 1 - input:28*28 , output : 30 + Activation Fucntion - Relu  + Batch Norm\n",
        "\n",
        "Layer 2 - input: 30 output : 10\n",
        "\n",
        "Cross Entropy Loss  + **Adam** optimizer "
      ]
    },
    {
      "metadata": {
        "id": "rpakqWPv9cs0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MNIST_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNIST_Net, self).__init__()\n",
        "        # an affine operation: y = Wx + b\n",
        "        self.fc0 = nn.Linear(28*28,30) #Layer 1\n",
        "        self.bn0 = nn.BatchNorm1d(30) # BatchNorm \n",
        "        self.fc1 =  nn.Linear(30, 10) # Layer 2\n",
        "    def forward(self, x):\n",
        "       \n",
        "        x = x.view(-1,28*28)\n",
        "        x = F.relu(self.bn0(self.fc0(x))) # Layer 1\n",
        "        x = self.fc1(x) # Layer 2\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "suyt84BXHzE9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VHANHwQT9c6N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mnist_net = MNIST_Net().cuda() # 생성한 뉴럴넷 Instance를 생성하고 빠른 학습을 위해 cuda 에 올립니다. \n",
        "criterion = nn.CrossEntropyLoss() # Loss Function을 정의 합니다. 여기서는 cross entrophy loss 를 사용합니다. \n",
        "optimizer = optim.Adam(mnist_net.parameters(), lr=0.001) # optimizer는 이와 같이 training 할 Parameter와 learning rate를 인자로 줍니다. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "el8EvYky9dgz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "296307dd-2dce-4008-b6fa-266857dbb2cf"
      },
      "cell_type": "code",
      "source": [
        "train_network(mnist_net,optimizer,trainDataLoader)"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,   500] loss: 0.896\n",
            "[1,  1000] loss: 0.366\n",
            "[1,  1500] loss: 0.293\n",
            "[2,   500] loss: 0.234\n",
            "[2,  1000] loss: 0.217\n",
            "[2,  1500] loss: 0.200\n",
            "[3,   500] loss: 0.172\n",
            "[3,  1000] loss: 0.183\n",
            "[3,  1500] loss: 0.176\n",
            "[4,   500] loss: 0.150\n",
            "[4,  1000] loss: 0.157\n",
            "[4,  1500] loss: 0.150\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IIUIeYYe9rid",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "6d5f94e9-7a20-4a33-f6d5-69baa02f6c4b"
      },
      "cell_type": "code",
      "source": [
        "test(mnist_net,testDataLoader)"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set:  Accuracy: 9620/10000 (96%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mBTuBB9_93mq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 간단한 Neural Network 를 만들어 봅시다. (8) 더 깊은 레이어에 Batch Norm 을 줘 볼까요?\n",
        "특징 : 2개의 Layer를 가지는 Neural Network\n",
        "\n",
        "<구성>  \n",
        "\n",
        "Layer 1 - input:28*28 , output : 40 + Activation Fucntion - Relu + BatchNorm\n",
        "\n",
        "Layer 2 - input: 40 output: 30 + Activation Fucntion - Relu  + BatchNorm\n",
        "\n",
        "Layer 3 - input: 30 output : 10\n",
        "\n",
        "Cross Entropy Loss  + **Adam** optimizer "
      ]
    },
    {
      "metadata": {
        "id": "qgGaaFW8-Ny7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MNIST_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNIST_Net, self).__init__()\n",
        "        # an affine operation: y = Wx + b\n",
        "        self.fc0 = nn.Linear(28*28,40) #Layer 1 \n",
        "        self.bn0 = nn.BatchNorm1d(40) #BatchNorm1 \n",
        "        self.fc1 = nn.Linear(40, 30) # Layer 2\n",
        "        self.bn1 = nn.BatchNorm1d(30) #BatchNorm1 \n",
        "        self.fc2 = nn.Linear(30, 10) # Layer 3\n",
        "\n",
        "    def forward(self, x):\n",
        "       \n",
        "        x = x.view(-1,28*28)\n",
        "        x = F.relu(self.bn0(self.fc0(x))) # Layer 1\n",
        "        x = F.relu(self.bn1(self.fc1(x))) # Layer 2\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ClLHQwZE-OCZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mnist_net = MNIST_Net().cuda() # 생성한 뉴럴넷 Instance를 생성하고 빠른 학습을 위해 cuda 에 올립니다. \n",
        "criterion = nn.CrossEntropyLoss() # Loss Function을 정의 합니다. 여기서는 cross entrophy loss 를 사용합니다. \n",
        "optimizer = optim.Adam(mnist_net.parameters(), lr=0.001) # optimizer는 이와 같이 training 할 Parameter와 learning rate를 인자로 줍니다. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KxHuM-IF-nz2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "2337bee3-ddf3-46de-a931-685d518b2b12"
      },
      "cell_type": "code",
      "source": [
        "train_network(mnist_net,optimizer,trainDataLoader)"
      ],
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,   500] loss: 0.845\n",
            "[1,  1000] loss: 0.282\n",
            "[1,  1500] loss: 0.220\n",
            "[2,   500] loss: 0.164\n",
            "[2,  1000] loss: 0.159\n",
            "[2,  1500] loss: 0.155\n",
            "[3,   500] loss: 0.135\n",
            "[3,  1000] loss: 0.128\n",
            "[3,  1500] loss: 0.121\n",
            "[4,   500] loss: 0.103\n",
            "[4,  1000] loss: 0.115\n",
            "[4,  1500] loss: 0.109\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xK1kspbG-qB9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "f3ca70b4-dc8f-4b28-aac6-a45c36ee1fce"
      },
      "cell_type": "code",
      "source": [
        "test(mnist_net,testDataLoader)"
      ],
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set:  Accuracy: 9707/10000 (97%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xxghrniJ_JIm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Batch Normalization 을 적용한 (7)과 (6)을 비교해보고 (8) 과 (5)를 비교해보면 어떻나요? 학습이 어떻게 달라졌을까요? "
      ]
    },
    {
      "metadata": {
        "id": "vGwSFl9v_c4F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Let's Do it - 성능을 한번 끝까지 높여볼까요~? 마음대로 한번 최고 성능을 찍어봅시다"
      ]
    },
    {
      "metadata": {
        "id": "fjM6p0gvkccx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MNIST_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNIST_Net, self).__init__()\n",
        "        \n",
        "        self.fc0 = nn.Linear(28*28,16*5*5)\n",
        "        self.bn0 = nn.BatchNorm1d(16*5*5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 84)\n",
        "        self.bn1 = nn.BatchNorm1d(84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "      \n",
        "        # If the size is a square you can only specify a single number\n",
        "        x = x.view(-1,28*28)\n",
        "        x = F.relu(self.bn0(self.fc0(x)))\n",
        "        x = F.relu(self.bn1(self.fc1(x)))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qGsCsD6blqBt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mnist_net = MNIST_Net().cuda()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(mnist_net.parameters(), lr=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3wPzE5N0lq-Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "98837cb0-c649-4632-aa48-60cb54de0e18"
      },
      "cell_type": "code",
      "source": [
        "train_network(mnist_net,optimizer,trainDataLoader)"
      ],
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,   500] loss: 0.469\n",
            "[1,  1000] loss: 0.182\n",
            "[1,  1500] loss: 0.149\n",
            "[2,   500] loss: 0.105\n",
            "[2,  1000] loss: 0.100\n",
            "[2,  1500] loss: 0.096\n",
            "[3,   500] loss: 0.062\n",
            "[3,  1000] loss: 0.070\n",
            "[3,  1500] loss: 0.071\n",
            "[4,   500] loss: 0.049\n",
            "[4,  1000] loss: 0.056\n",
            "[4,  1500] loss: 0.056\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bhuKrwYElsAj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "7f4b393e-c283-47f5-9f07-a07a5fc7cdea"
      },
      "cell_type": "code",
      "source": [
        "test(mnist_net,testDataLoader)"
      ],
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set:  Accuracy: 9791/10000 (98%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Xxip6TQc_txE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Practical Guide Pytorch nn.Sequential "
      ]
    },
    {
      "metadata": {
        "id": "SDexfQHd_72G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "x = F.relu(self.bn0(self.fc0(x)))\n",
        "x = F.relu(self.bn1(self.fc1(x)))\n",
        "```\n",
        "너무 복잡하지 않나요?  그냥 x = self.fc(x) 쉽게 해버리면 안 될까요?\n",
        "\n",
        "Solution : nn.Sequential + 자매품 nn.ModuList\n"
      ]
    },
    {
      "metadata": {
        "id": "59l4rpz5ls39",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MNIST_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNIST_Net, self).__init__()\n",
        "        \n",
        "        layer_list = [] # 이 리스트에 모든 Layer 를 순차적으로 append 해보겠습니다. \n",
        "        layer_list.append(nn.Linear(28*28,40)) #Layer 1 \n",
        "        layer_list.append(nn.BatchNorm1d(40))#BatchNorm1 \n",
        "        layer_list.append(nn.Linear(40, 30)) # Layer 2\n",
        "        layer_list.append(nn.BatchNorm1d(30)) #BatchNorm1 \n",
        "        layer_list.append(nn.Linear(30, 10)) # Layer 3\n",
        "        self.net  = nn.Sequential(*layer_list) # nn.Sequential 에 layer list를 넘겨 줍니다.\n",
        "    def forward(self, x):\n",
        "       \n",
        "        x = x.view(-1,28*28)\n",
        "        x = self.net(x) # 넣은 순서대로 적용이 됩니다. \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NRRIS8y1mOOK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mnist_net = MNIST_Net().cuda() # 생성한 뉴럴넷 Instance를 생성하고 빠른 학습을 위해 cuda 에 올립니다. \n",
        "criterion = nn.CrossEntropyLoss() # Loss Function을 정의 합니다. 여기서는 cross entrophy loss 를 사용합니다. \n",
        "optimizer = optim.Adam(mnist_net.parameters(), lr=0.001) # optimizer는 이와 같이 training 할 Parameter와 learning rate를 인자로 줍니다. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CF5Tr8dXBMmg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "9071bdcc-cd2b-4eb0-d0fe-12409dea7f0b"
      },
      "cell_type": "code",
      "source": [
        "train_network(mnist_net,optimizer,trainDataLoader)"
      ],
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,   500] loss: 0.728\n",
            "[1,  1000] loss: 0.398\n",
            "[1,  1500] loss: 0.375\n",
            "[2,   500] loss: 0.338\n",
            "[2,  1000] loss: 0.344\n",
            "[2,  1500] loss: 0.336\n",
            "[3,   500] loss: 0.328\n",
            "[3,  1000] loss: 0.317\n",
            "[3,  1500] loss: 0.320\n",
            "[4,   500] loss: 0.313\n",
            "[4,  1000] loss: 0.308\n",
            "[4,  1500] loss: 0.326\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "b7o8XOAlBSFB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "c0c99ddf-7c26-4ec5-917e-eb283e012745"
      },
      "cell_type": "code",
      "source": [
        "test(mnist_net,testDataLoader)"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set:  Accuracy: 9189/10000 (92%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ysv7uX-1Dh0b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 연습해 봅시다 ! \n",
        "\n",
        "특징 : 2개의 Layer를 가지는 Neural Network <구성>\n",
        "\n",
        "Layer 1 - input:28*28 , output : 30 + Activation Fucntion - Relu + Batch Norm\n",
        "\n",
        "Layer 2 - input: 30 output : 10\n",
        "\n",
        "Cross Entropy Loss + Adam optimizer"
      ]
    },
    {
      "metadata": {
        "id": "-esUcJ8VDu_q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MNIST_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNIST_Net, self).__init__()\n",
        "        \n",
        "        layer_list = [] # 이 리스트에 모든 Layer 를 순차적으로 append 해보겠습니다. \n",
        "        layer_list.append(nn.Linear(28*28,30)) #Layer 1 \n",
        "        layer_list.append(nn.BatchNorm1d(30)) #BatchNorm1 \n",
        "        layer_list.append(nn.Linear(30, 10)) # Layer 2\n",
        "        self.net  = nn.Sequential(*layer_list) # nn.Sequential 에 layer list를 넘겨 줍니다.\n",
        "    def forward(self, x):\n",
        "       \n",
        "        x = x.view(-1,28*28)\n",
        "        x = self.net(x) # 넣은 순서대로 적용이 됩니다. \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jaktO6PvD0M4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mnist_net = MNIST_Net().cuda() # 생성한 뉴럴넷 Instance를 생성하고 빠른 학습을 위해 cuda 에 올립니다. \n",
        "criterion = nn.CrossEntropyLoss() # Loss Function을 정의 합니다. 여기서는 cross entrophy loss 를 사용합니다. \n",
        "optimizer = optim.Adam(mnist_net.parameters(), lr=0.001) # optimizer는 이와 같이 training 할 Parameter와 learning rate를 인자로 줍니다. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9She_4cMD1ES",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "0146b8cf-77b0-491f-c5f1-68b11a03f6d3"
      },
      "cell_type": "code",
      "source": [
        "train_network(mnist_net,optimizer,trainDataLoader)"
      ],
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,   500] loss: 0.760\n",
            "[1,  1000] loss: 0.383\n",
            "[1,  1500] loss: 0.352\n",
            "[2,   500] loss: 0.330\n",
            "[2,  1000] loss: 0.329\n",
            "[2,  1500] loss: 0.324\n",
            "[3,   500] loss: 0.305\n",
            "[3,  1000] loss: 0.308\n",
            "[3,  1500] loss: 0.323\n",
            "[4,   500] loss: 0.301\n",
            "[4,  1000] loss: 0.302\n",
            "[4,  1500] loss: 0.313\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kseseeW4D1zc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "ccccb43b-17f0-4da2-aac3-e463a65901b3"
      },
      "cell_type": "code",
      "source": [
        "test(mnist_net,testDataLoader)"
      ],
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set:  Accuracy: 9204/10000 (92%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}